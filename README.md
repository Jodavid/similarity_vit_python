# ğŸ’» Similaridade de Imagens usando Vision Transformers no python

Ãštima AtualizaÃ§Ã£o: 05.08.2025

<div align="center">
<img src="https://img.shields.io/badge/Python-%233776AB.svg?&style=for-the-badge&logo=python&logoColor=white" alt="Python">
<img src="https://img.shields.io/badge/Machine%20Learning-%23FF6F00.svg?&style=for-the-badge&logo=google-colab&logoColor=white" alt="ML">
<img src="https://img.shields.io/badge/Deep%20Learning-%230075A8.svg?&style=for-the-badge&logo=tensorflow&logoColor=white" alt="DL">
</div>

Esse repositÃ³rio contÃ©m a apresentaÃ§Ã£o e cÃ³digos do tutorial "Similaridade de Imagens usando Vision Transformers no python" que foi apresentado no RBRAS 2025.

![](apresentacao/images/logo_capa.png)

## Sobre o tutorial:


Neste tutorial, serÃ¡ utilizado uma abordagem para a correspondÃªncia de imagens utilizando o Vision Transformer (ViT) (<https://arxiv.org/abs/2010.11929>). O objetivo Ã© demonstrar uma tÃ©cnica de deep learning para realizar correspondÃªncia de imagens, que se refere ao processo de identificar e emparelhar partes semelhantes ou correspondentes em duas ou mais imagens. No entanto, devido a variaÃ§Ãµes significativas de iluminaÃ§Ã£o, Ã¢ngulo de captura e qualidade da imagem, esta tarefa se torna complexa, especialmente em cenÃ¡rios do mundo real, como o reconhecimento de objetos, animais ou pessoas em diferentes imagens.

A arquitetura ViT adapta o modelo Transformer - originalmente desenvolvido para tarefas de Processamento de Linguagem Natural (NLP) - para o processamento de dados de imagem. Ela utiliza embeddings, que sÃ£o vetores que capturam a informaÃ§Ã£o semÃ¢ntica de palavras, frases ou imagens, permitindo mapear dados semelhantes para pontos prÃ³ximos em um espaÃ§o vetorial de alta dimensionalidade. Na correspondÃªncia de imagens, o desafio estÃ¡ em como representar as imagens de forma eficaz como embeddings, capturando caracterÃ­sticas relevantes. Ao aplicar tÃ©cnicas de prÃ©-processamento, como a remoÃ§Ã£o de fundo, buscamos isolar o objeto de interesse, o que auxilia em tarefas como reconhecimento de objetos, segmentaÃ§Ã£o de imagens e correspondÃªncia.

A partir dos embeddings, a similaridade entre diferentes imagens Ã© calculada utilizando a similaridade do cosseno. A imagem de interesse Ã© comparada a uma base de dados de imagens, e os resultados sÃ£o retornados com um score de similaridade. O score de similaridade do cosseno quantifica a semelhanÃ§a entre duas imagens em termos de suas caracterÃ­sticas extraÃ­das. Esta mÃ©trica varia de -1 a 1, onde, quanto mais prÃ³ximo de 1, mais alinhados estÃ£o os vetores que representam as imagens, indicando que elas compartilham muitas caracterÃ­sticas comuns. Um valor de 0 indica que os vetores de embeddings das imagens sÃ£o ortogonais, o que significa que as imagens tÃªm pouco em comum, e quanto mais prÃ³ximo de -1, mais opostos sÃ£o os vetores, significando que as imagens sÃ£o muito diferentes.

Assim, este tutorial tem com objetivo abordar as etapas de prÃ©-processamento de imagens, geraÃ§Ã£o de embeddings usando Vision Transformers e cÃ¡lculo de similaridade atravÃ©s da similaridade do cosseno. Ao final, os participantes entenderÃ£o como aplicar esses mÃ©todos tanto em contextos acadÃªmicos quanto prÃ¡ticos.

*Link do modelo no Hugging-Face:* https://huggingface.co/google/vit-base-patch16-224

*Palavras-chave:*
Vision Transformers, similaridade de imagens, deep learning, embeddings, visÃ£o computacional


## Estrutura do RepositÃ³rio

```
similarity_vit_python/
 â”œâ”€â”€ apresentacao/
 â”‚   â””â”€â”€ index.html
 â””â”€â”€ Codigo_ViT/
     â”œâ”€â”€ Pre_processamento_ViT_com_dados_imagens_digitais.ipynb
     â””â”€â”€ ViT_com_dados_imagens_digitais_Match_Images.ipynb
```