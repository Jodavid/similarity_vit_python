<!DOCTYPE html>
<html lang="en"><head>
<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/tabby.min.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.40">

  <meta name="author" content="Jodavid Ferreira">
  <title>index</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/theme/quarto-bbe7401fe57d4b791b917637bb662036.css">
  <link rel="stylesheet" href="css/jodavid.css">
  <link href="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section id="section" class="slide level2" data-background-image="images/capa.png" data-background-opacity="1" data-background-size="80%">
<h2></h2>
<div style="margin-left: -100px;margin-top: -170px;">
<p><img src="images/logo_capa.png" width="500px"></p>
</div>
</section>
<section id="section-1" class="slide level2" data-background-image="images/capa_pb.png" data-background-opacity=".1" data-background-size="80%">
<h2></h2>
<p><br> <br></p>
<div style="margin-top:20px">
<h2 style="color:#0000fd;">
Similaridade de Imagens usando Vision Transformers no python
</h2>
</div>
<br> <br>
<h3>
Jodavid Ferreira
</h3>
<h4>
Departamento de Estatística
</h4>
<h4>
Universidade Federal de Pernambuco
</h4>

<!---

<br/> <br/> <br/> <br/> <br/>



![](images/imagem1.jpeg){.absolute bottom="180" right="0" width="700"}

<h2 style="text-align: left"> O que é </h2>
<h2 style="text-align: left"> Ciência de </h2>
<h2 style="text-align: left"> Dados? </h2>

--->
<img data-src="images/logo_ufpe_facepe.png" class="r-stretch"></section>
<section id="section-2" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Início
</h2>
<hr>
<p><br></p>
<ul>
<li>Maio/2024;</li>
<li>Enchente no Rio Grande do Sul;</li>
</ul>
<div class="fragment">
<div style="text-align:left;">
<p><img data-src="images/RioGrandedoSul.jpg" class="fixed" width="500"></p>
</div>
</div>
<div class="fragment">
<div style="text-align:right; margin-top:-500px; margin-right:10px;">
<ul>
<li>Muitas pessoas desabrigadas;</li>
<li>Muitos animais perdidos de seus donos;</li>
</ul>
</div>
</div>
<div class="fragment">
<div style="text-align:right;">
<p><img data-src="images/Cachorro-enchente.jpg" class="fixed" width="500"></p>
</div>
</div>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Ideia neste tutorial
</h2>
<hr>
<div style="text-align:center;">
<img data-src="images/scraping.png" class="fixed" width="900">
<h3>
Fazer Scraping de Imagens
</h3>
</div>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Ideia neste tutorial
</h2>
<hr>
<div style="text-align:center;">
<p><img data-src="images/montagem_rbras_2025/rect8409_1.png" class="fixed" style="height:100.0%"></p>
</div>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Ideia neste tutorial
</h2>
<hr>
<p><img data-src="images/montagem_rbras_2025/rect8409_2.png" class="fixed" style="width:90.0%"></p>
<div class="fragment">
<div style="margin-left: 130px; margin-top: -500px;">
<p><img data-src="images/montagem_rbras_2025/rect8409_0.png" class="fixed" width="700"></p>
</div>
</div>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Ideia neste tutorial
</h2>
<hr>

<img data-src="images/montagem_rbras_2025/rect8409_3.png" class="fixed r-stretch" style="width:100.0%"></section>
<section id="section-7" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Ideia neste tutorial
</h2>
<hr>

<img data-src="images/montagem_rbras_2025/rect8409_4.png" class="fixed r-stretch" style="width:100.0%"></section>
<section id="section-8" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Desafio com imagens
</h2>
<hr>
<p><br></p>
<div style="text-align:center;">
<div class="columns">
<div class="column" style="width:48%;">
<p><img data-src="images/gato3.jpeg" class="fixed" width="500"></p>
</div><div class="column" style="width:48%;">
<p><img data-src="images/gato4.jpeg" class="fixed" width="500"></p>
</div></div>
</div>
<h3 style="text-align:center; color:red;">
Como posso garantir que nessas duas imagens estão o mesmo animal?
</h3>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Imagens digitais
</h2>
<hr>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<img data-src="images/visao/image_original.png" class="absolute" style="left: 15%; width: 300px; ">
<div style="margin-left: 200px; margin-top: 230px;">
Imagem Original
</div>
<img data-src="images/visao/componente_g.png" class="absolute" style="left: 15%; bottom: 5%; width: 300px; ">
<div style="margin-left: 200px; margin-top: 230px;">
Componente - G
</div>
</div><div class="column" style="width:50%;">
<img data-src="images/visao/componente_R.png" class="absolute" style="right: 15%; width: 300px; ">
<div style="margin-left: 130px; margin-top: 230px;">
Componente - R
</div>
<img data-src="images/visao/componente_b.png" class="absolute" style="bottom: 5%; right: 15%; width: 300px; ">
<div style="margin-left: 130px; margin-top: 230px;">
Componente - B
</div>
</div></div>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Imagens digitais
</h2>
<hr>
<p><br></p>
<h4 id="espaço-rgb-de-cores">Espaço RGB de Cores</h4>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="images/visao/imagem_rgb.png" class="absolute" style="width: 450px; "></p>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Um único pixel consiste de três componentes que variam entre [0,255].</p></li>
<li><p>Cada <em>pixel</em> e um vetor:</p></li>
</ul>
<p><img data-src="images/visao/vetor_computador.png" class="absolute" style="width: 550px; "></p>
<p><br> <br> <br> <br></p>
<!-- <div style="text-align:center; margin: -30px 200px 0px 50px;">
Vetor-pixel na
memória do
computador
</div> -->
<div style="text-align:center; margin: -30px 0px 0px 400px;">
<p>Pixel na imagem</p>
</div>
</div></div>
</section>
<section id="section-11" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Imagens digitais
</h2>
<hr>
<p><br></p>
<ul>
<li><p>Em geral define-se em três como o número de cores primarias em um espaço, devido ao fato do olho humano possuírem três tipos de fotorreceptores.</p></li>
<li><p>A partir destas cores primarias, é possível gerar todas as outras cores do espaço.</p></li>
</ul>
<p><strong>Representação como pontos de um espaço 3D de Cor</strong></p>
<p>Cores criadas com o vetor R,G,B</p>
<div style="text-align:center;">
<p><img data-src="images/visao/tabela_cores_exemplo.png" class="fixed" width="400"></p>
</div>
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Imagens digitais
</h2>
<hr>
<p>Entretanto, existem outras formas de representar cores, como o espaço CMYK.</p>
<ul>
<li><p>O padrão RGB tem síntese aditiva, e é conhecido como cor luz, pois quando as três cores são sobrepostas formam o branco. Já o CMY tem síntese substrativa (também conhecido como pigmento), pois quando sobrepostas, as três cores formam a cor preto (K).</p>
<ul>
<li>Que são as cores que são utilizadas em impressoras.</li>
</ul></li>
</ul>
<div class="fragment">
<div style="margin-left: 130px; margin-top: -150px;">
<p><img data-src="images/visao/cmyk-and_rgb.webp"></p>
</div>
</div>
</section>
<section class="slide level2">

<p><br> <br> <br> <br> <br></p>
<hr>
<h2 style="text-align:center; color:#0000fd;">
Processamento de Imagens com
</h2>
<h2 style="text-align:center;  color:#0000fd;">
<em>Deep Learning</em>
</h2>
<hr>
<p><br></p>
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Redes Neurais Convolucionais
</h2>
<hr>
<p><br></p>
<h4 id="um-pouco-de-história">Um pouco de história</h4>
<ul>
<li>O <strong>ImageNet Large-Scale Visual Recognition Challenge (ILSVRC)</strong> era uma competição anual de reconhecimento visual em larga escala, que começou em 2010 e ocorreu até 2017. A competição era baseada no banco de dados ImageNet, que contém milhões de imagens anotadas em milhares de categorias.</li>
</ul>
<div class="columns">
<div class="column" style="width:60%;">
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/ImageNet.jpg" class="fixed" style="width:35.0%"></p>
<figcaption>Fonte: https://cs.stanford.edu/people/karpathy/cnnembed/</figcaption>
</figure>
</div>
</div>
</div><div class="column" style="width:40%;">
<ul>
<li>O dataset ImageNet contém 14.197.122 imagens rotuladas, com 20.000 categorias de objetos, sendo que cada categoria contém pelo menos 500 imagens.</li>
</ul>
</div></div>
</section>
<section id="section-14" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Redes Neurais Convolucionais
</h2>
<hr>
<p><br></p>
<h4 id="um-pouco-de-história-1">Um pouco de história</h4>
<ul>
<li>Em 2012, a equipe da Universidade de Toronto, liderada por Alex Krizhevsky, Ilya Sutskever e Geoffrey Hinton, desenvolveu uma CNN chamada <strong>AlexNet</strong> (<a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">paper</a>), que obteve uma precisão de erro de 16,4%, superando significativamente os métodos tradicionais e significativamente melhor que o segundo colocado do mesmo ano, que teve uma taxa de erro de 26,2%.</li>
</ul>
<div style="text-align:center; margin-top: -50px;">
<p><img data-src="images/cnn/CNN.gif" class="fixed" style="width:55.0%"></p>
</div>
</section>
<section id="section-15" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Redes Neurais Convolucionais
</h2>
<hr>
<p><br></p>
<ul>
<li>As Redes Neurais Convolucionais (CNNs) continuaram atraindo a atenção após vencerem o Desafio ImageNet até o ano de 2017.</li>
</ul>
<!-- - Existiam 50.000 imagens coloridas de alta resolução em 1.000 categorias;

- O treinamento com 1,2 milhão de imagens; -->
<ul>
<li>Em 2017, a SENet (<a href="https://arxiv.org/abs/1709.01507" class="uri">https://arxiv.org/abs/1709.01507</a>) alcançou uma taxa de erro de 2,3% em 2017.</li>
</ul>
<div style="text-align:center;">
<p><img data-src="images/cnn/top_five_imagenet.png" class="fixed" style="width:65.0%"></p>
</div>
</section>
<section id="section-16" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Redes Neurais Convolucionais
</h2>
<hr>

<img data-src="images/cnn/arquitetura_cnn.png" class="fixed r-stretch" style="width:60.0%"><ul>
<li>A Figura acima mostra uma arquitetura típica de uma rede neural convolucional que contém uma camada de entrada, camadas convolucionais, camadas de pooling (subamostragem, ou down sampling), camadas de ativação, camadas totalmente conectadas e uma camada de saída.</li>
</ul>
</section>
<section id="section-17" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
ViT - Visual Transformers
</h2>
<hr>
<p><br></p>
<ul>
<li><p><strong>Junho/2021:</strong> Autores da Google Research publicaram o artigo “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” (<a href="https://arxiv.org/abs/2010.11929">paper</a>).</p></li>
<li><p>Segundo os autores, inspirados pelos sucessos do escalonamento de Transformers em NLP (Processamento de Linguagem Natural), experimentaram aplicar um Transformer padrão diretamente às imagens, com o mínimo de modificações possível.</p></li>
</ul>
<!-- Para isso, dividimos uma imagem em patches e fornecemos a sequência de embeddings lineares desses patches como entrada para um Transformer. Patches de imagem são tratados da mesma forma que tokens (palavras) em uma aplicação de NLP. Treinamos
o modelo em classificação de imagens de forma supervisionada. -->
<div style="text-align:center;">
<p><img data-src="images/arquiteturaViT.png" class="fixed" style="width:50.0%"></p>
</div>
</section>
<section id="section-18" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
ViT - Visual Transformers
</h2>
<hr>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/ch13-fig04.png" class="fixed" style="width:80.0%"></p>
<figcaption>Como um vision transformer opera sobre os patches das imagens.</figcaption>
</figure>
</div>
</div>
<!-- Figura 1: Visão geral do modelo. Dividimos uma imagem em fragmentos de tamanho fixo, incorporamos linearmente cada um deles, adicionamos embeddings de posição e alimentamos a sequência de vetores resultante em um codificador Transformer padrão. Para realizar a classificação, usamos a abordagem padrão de adicionar um "token de classificação" extra aprendível à sequência. A ilustração do codificador Transformer foi inspirada em Vaswani et al. -->
</section>
<section id="section-19" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Tokens e Embeddings
</h2>
<hr>
<ul>
<li><em>Tokens</em> e <em>Embeddings</em> são a base dos modelos baseados em atenção e <em>transformers</em>.</li>
</ul>
<div class="fragment">
<p>No contexto textual, a <strong>tokenização</strong> é o processo de pegar o texto e transformar as sequências de entrada em representação numérica.</p>
<ul>
<li>é um mapeamento direto de <em>palavras</em> para números, uma mesma palavra vai receber o mesmo <em>token</em> (pode ser modelado, mas rapidamente se torna muito grande).</li>
<li>os <em>tokens</em> geralmente são palavras, mas também podem ser frases, sinais de pontuação ou até caracteres individuais.</li>
<li>A tokenização é o primeiro passo no processamento de linguagem natural (NLP) e é essencial para a pré-processamento de texto.</li>
<li>ela ajuda a preparar os dados textuais para análise, tornando-os mais estruturados.</li>
</ul>
</div>
</section>
<section id="section-20" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Tokens e Embeddings
</h2>
<hr>
<p><img data-src="images/token2.png" style="margin: 0 0 0 300px; width: 450px; height: auto;"></p>
<p>Apesar da referência, palavras grandes podem ser divididas em <em>subtokens</em> menores, sendo assim, em 1.000 tokens de palavras em português correspondem aproximadamente a aproximadamente 700 a 750 palavras do nosso idioma.</p>
<p>Essa contagem de palavras em um texto pode variar dependendo da linguagem, do tamanho das palavras e do uso de pontuações.</p>
<div style="align-items: center;">
<p><img data-src="images/token_embeddings.gif" style="margin: 0 0 0 300px; width: 450px; height: auto;"></p>
</div>
</section>
<section id="section-21" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Tokens e Embeddings
</h2>
<hr>
<p><br></p>
<ul>
<li><p><em>Embeddings</em> são vetores numéricos obtidos dos <em>tokens</em> e representam palavras, frases ou documentos.</p></li>
<li><p>Os <em>embeddings</em> é o processo de transformar o mapeamento do vetor de texto de entrada em uma representação matricial<sup>1</sup>.</p></li>
<li><p>Os <em>embeddings</em> possuem uma melhor representação do relacionamento entre os tokens.</p></li>
<li><p>Os <em>embeddings</em> conseguem capturar a estrutura semântica das palavras ou frases e suas relações no texto.</p></li>
<li><p>Atualmente, elas são criadas usando técnicas de <em>machine learning</em>, como Word2Vec ou GloVe e <em>deep learning</em>, como BERT, GPT-3, e os modelos mais atuais de LLMs.</p></li>
</ul>
<!-- ---


## 
<h2 style= "color:#0000fd;"> Tokens e Embeddings</h2>

<hr/>


![](images/embedd2.webp) -->
<aside><ol class="aside-footnotes"><li id="fn1"><p>Alguns modelos já incorporam o processo de tokenização.</p></li></ol></aside></section>
<section id="section-22" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Tokens e Embeddings
</h2>
<hr>
<ul>
<li><p>Os embeddings podem ter um número maior ou menor de dimensões do que a entrada original. Por exemplo, usando métodos de embedding para expressão extrema, podemos codificar dados em representações bidimensionais densas e contínuas para fins de visualização.</p></li>
<li><p>Uma propriedade fundamental dos embeddings é que eles codificam distância ou similaridade, ou seja, eles capturam a semântica dos dados de forma que entradas semelhantes estejam próximas no espaço de embeddings.</p></li>
</ul>
<div style="text-align:center;">
<p><img data-src="images/ch01-fig02.png" class="fixed" style="width:50.0%"></p>
</div>
</section>
<section id="section-23" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
ViT: Uma ideia inspirada no BERT
</h2>
<hr>
<p><br></p>
<p>O <strong>Vision Transformer (ViT)</strong> aplica uma ideia originalmente desenvolvida para <strong>texto</strong> — o <strong>BERT</strong> — no mundo da <strong>visão computacional</strong>.</p>
<p><br></p>
<p>Ambos os modelos usam o mesmo conceito central:<br>
- <strong>“Compreender partes (tokens ou patches) em relação ao todo usando atenção.”</strong></p>
<div class="fragment">
<p><br></p>
<h3 style="color:#0000fd;">
O que é o BERT?
</h3>
<p><strong>BERT</strong> (<em>Bidirectional Encoder Representations from Transformers</em>) é um modelo de linguagem criado pela <a href="https://research.google/blog/open-sourcing-bert-state-of-the-art-pre-training-for-natural-language-processing/">Google (2018)</a> para <strong>compreender texto com base no contexto completo</strong>.</p>
</div>
</section>
<section id="section-24" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
BERT
</h2>
<hr>
<p><br></p>
<ul>
<li><p>o BERT <strong>olha para toda a frase ao mesmo tempo</strong> (bidirecionalidade)</p>
<ul>
<li>ao contrário de modelos que leem da <strong>esquerda para a direita</strong>,</li>
</ul></li>
<li><p>Isso permite entender o <strong>significado exato de uma palavra</strong>, considerando tudo o que está antes <strong>e depois</strong> dela.</p>
<ul>
<li>Frase 1: “<em>Ele sentou no banco para descansar.</em>”<br>
</li>
<li>Frase 2: “<em>Ele foi ao banco sacar dinheiro.</em>”</li>
</ul></li>
<li><p>Mesmo a palavra <strong>“banco”</strong> sendo igual, o significado muda.</p></li>
<li><p>O BERT entende isso porque ele <strong>considera todas as palavras da frase ao mesmo tempo</strong>.</p></li>
</ul>
</section>
<section id="section-25" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Como o BERT funciona internamente?
</h2>
<hr>
<p><br></p>
<p>O BERT é baseado no <strong>Encoder do Transformer</strong>. Ele utiliza:</p>
<p><br></p>
<ul>
<li><strong>Tokenização</strong>: cada palavra vira um token.</li>
<li><strong>Embeddings</strong>: os tokens viram vetores numéricos.</li>
<li><strong>Atenção (Self-Attention)</strong>: calcula o quanto cada palavra influencia as outras.</li>
<li><strong>Tarefa de treino</strong>: <em>Masked Language Modeling (MLM)</em>, onde um percentual de palavras (por exemplo, 15%) são ocultadas e o modelo precisa prever quais são.</li>
</ul>
</section>
<section id="section-26" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
A ideia central: tokens + atenção
</h2>
<hr>
<p><br></p>
<blockquote>
<p>“Vamos representar uma sequência de unidades (tokens, patches) como vetores e deixar o modelo <strong>aprender relações entre elas com autoatenção</strong>.”</p>
</blockquote>
<p><br></p>
<p>Essa ideia permite que o modelo <strong>reconheça padrões globais</strong>, não apenas locais.</p>
<p><br></p>
<p>O ViT <strong>usa exatamente essa mesma lógica</strong>, mas com <strong>imagens</strong> em vez de texto.</p>
<!-- 

---

## 
<h2 style= "color:#0000fd;"> ViT e BERT</h2>

<hr/>
<br/>

| Etapa                   | BERT (Texto)                      | ViT (Imagem)                      |
|-------------------------|------------------------------------|-----------------------------------|
| Entrada                 | Sequência de palavras (tokens)     | Patches da imagem (ex: 16×16 px)  |
| Embedding               | Vetor para cada palavra            | Vetor para cada patch             |
| Posição                 | Embedding de posição adicionado    | Embedding de posição adicionado   |
| Processamento           | Transformer Encoder (atenção)      | Transformer Encoder (atenção)     |
| Pré-treinamento         | Prever palavras (MLM)              | Prever patches (Masked Patch)     |
| Saída                   | Tarefa textual                     | Tarefa visual (ex: classificação) |

--- -->
</section>
<section id="section-27" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
ViT aplicado às imagens?
</h2>
<hr>
<p><br></p>
<p>O <strong>ViT</strong> (Vision Transformer) transforma uma imagem da seguinte forma:</p>
<p><br></p>
<ol type="1">
<li><strong>Divide a imagem em pequenos blocos</strong> (patches), como se fossem palavras visuais.</li>
<li>Cada patch é <strong>transformado em um vetor</strong> (embedding), como o BERT faz com palavras.</li>
<li>Esses vetores são alimentados em um <strong>Transformer Encoder</strong>, com camadas de atenção.</li>
<li>O modelo aprende a <strong>relacionar todas as partes da imagem</strong> para tomar decisões (ex: classificar).</li>
</ol>
</section>
<section id="section-28" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
ViT aplicado às imagens?
</h2>
<hr>
<p><br></p>
<p>O <strong>ViT</strong> (Vision Transformer) transforma uma imagem da seguinte forma:</p>
<p><br></p>
<p><span style="color:red;"> 1. <strong>Divide a imagem em pequenos blocos</strong> (patches), como se fossem palavras visuais. </span></p>
<p><span style="color:red;"> 2. Cada patch é <strong>transformado em um vetor</strong> (embedding), como o BERT faz com palavras. </span></p>
<p><span style="color:red;"> 3. Esses vetores são alimentados em um <strong>Transformer Encoder</strong>, com camadas de atenção. </span></p>
<ol start="4" type="1">
<li>O modelo aprende a <strong>relacionar todas as partes da imagem</strong> para tomar decisões (ex: classificar, segmentar).</li>
</ol>
</section>
<section id="section-29" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
FAISS
</h2>
<hr>
<p><br></p>
<blockquote>
<p><strong>FAISS (Facebook AI Similarity Search)</strong> é uma biblioteca criada pelo Facebook AI para fazer <strong>buscas eficientes em grandes conjuntos de vetores</strong> (como <em>embeddings</em>).</p>
</blockquote>
<div class="fragment">
<h4 style="color:#0000fd; font-size:30pt;">
Para que serve?
</h4>
<p>Quando você tem <strong>vetores</strong> (ex: gerados por BERT ou ViT) e deseja encontrar <strong>os mais próximos</strong> (similaridade).</p>
<p>Isso é comum em tarefas como:</p>
<ul>
<li>Busca semântica (texto/imagem semelhante)</li>
<li>Recuperação de imagens por conteúdo</li>
<li>Sistemas de recomendação baseados em embeddings <!-- - Deduplicação ou clustering rápido --></li>
</ul>
<!-- ---

## 
<h2 style= "color:#0000fd;">Distância do Cosseno</h2>

<hr/>

:::: {.columns}

::: {.column width="50%"}

<div style="font-size:18pt">

Detalhes importantes:

Similaridade do Cosseno ($Sim_{cos}$):

- Maior valor (próximo de 1): Maior similaridade.
- Menor valor (próximo de -1): Maior dissimilaridade.

Distância do Cosseno ($D_{cos} = 1 - Sim_{cos}$):

- Maior valor (próximo de 2): Maior dissimilaridade.
- Menor valor (próximo de 0): Maior similaridade.


</div>

:::

::: {.column width="3%"}
:::

::: {.column width="47%"}

<br/>

![](images/simi_eq.png){style="margin: 0 0 0 0; width:550px;"}
<br/>

![](images/Cosine-similarity.jpg){style="margin: 0 0 0 0; width:550px;"}

:::

::::


--- -->
</div>
</section>
<section id="section-30" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
k-NN - vizinhos mais próximos
</h2>
<h4 style="color:#0000fd;">
usando similaridade do cosseno
</h4>
<hr>
<ul>
<li>Quando trabalhamos com <strong>embeddings</strong> (vetores que representam texto, imagem, etc), uma tarefa comum é encontrar os <strong>k vetores mais semelhantes</strong> a um vetor consulta.</li>
</ul>
<p><br></p>
<h4 style="color:#0000fd;">
O que é a similaridade do cosseno?
</h4>
<div class="columns">
<div class="column" style="width:50%;">
<div style="font-size:18pt">
<ul>
<li>Mede o <strong>ângulo</strong> entre dois vetores.</li>
</ul>
<p>Similaridade do Cosseno:</p>
<ul>
<li>Maior valor (próximo de 1): Maior similaridade.</li>
<li>Menor valor (próximo de -1): Maior dissimilaridade.</li>
</ul>
<!-- Distância do Cosseno ($D_{cos} = 1 - Sim_{cos}$):

- Maior valor (próximo de 2): Maior dissimilaridade.
- Menor valor (próximo de 0): Maior similaridade. -->
</div>
</div><div class="column" style="width:3%;">

</div><div class="column" style="width:47%;">
<p><br></p>
<p><img data-src="images/simi_eq.png" style="margin: -100px 0 0 0; width:550px;"> <br></p>
<p><img data-src="images/Cosine-similarity.jpg" style="margin: 0 0 0 0; width:550px;"></p>
</div></div>
</section>
<section id="section-31" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
k-NN com similaridade do cosseno
</h2>
<h4 style="color:#0000fd;">
Como funciona?
</h4>
<hr>
<ol type="1">
<li>Dado um vetor consulta, comparamos ele com todos os vetores da base.</li>
<li>Calculamos a similaridade do cosseno entre o vetor consulta e cada vetor da base.</li>
<li>Selecionamos os <strong>k vetores com maior similaridade</strong> (mais próximos).</li>
</ol>
<p><br></p>
<h4 style="color:#0000fd;">
Por que usar similaridade do cosseno?
</h4>
<ul>
<li>Embeddings geralmente são normalizados para magnitude 1.</li>
<li>A similaridade do cosseno captura melhor a <strong>orientação</strong> dos vetores, que reflete similaridade semântica, independente do comprimento.</li>
</ul>
</section>
<section id="section-32" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Vamos ao python?
</h2>
<h4 style="color:#0000fd;">
Como vamos seguir:
</h4>
<hr>
<ol type="1">
<li><strong>Banco de dados de Imagens:</strong> A base de dados foi nomeada de <strong>Natural Images</strong> e pode ser encontrada no Kaggle e no Github <a href="https://github.com/prasunroy/cnn-on-degraded-images?tab=readme-ov-file" class="uri">https://github.com/prasunroy/cnn-on-degraded-images?tab=readme-ov-file</a>.</li>
</ol>
<div class="fragment">
<ol start="2" type="1">
<li><strong>Remover Background:</strong> Vamos remover o fundo das imagens para facilitar na similaridade das imagens. Foi utilizado o U²-Net <a href="https://arxiv.org/pdf/2005.09007" class="uri">https://arxiv.org/pdf/2005.09007</a></li>
</ol>
</div>
<div class="fragment">
<ol start="3" type="1">
<li><strong>Criar Embeddings:</strong> Vamos criar os embeddings das imagens usando o ViT.</li>
</ol>
</div>
<div class="fragment">
<ol start="4" type="1">
<li><strong>FAISS:</strong> Armazenar os embeddings no FAISS.</li>
</ol>
</div>
<div class="fragment">
<ol start="5" type="1">
<li><strong>Encontrar Similares:</strong> Vamos buscar imagens similares usando através do cosseno.</li>
</ol>
<!-- ---

## 
<h2 style= "color:#0000fd;"> Similaridades das *words*</h2>

<hr/>

<br/>

:::: {.columns}

::: {.column width="50%"}

<div style="font-size:18pt">

- As *word embeddings* transformam os valores inteiros únicos obtidos a partir do tokenizador em um array $n$-dimensional.

- Por exemplo, a palavra 'gato' pode ter o valor '20' a partir do tokenizador, mas a camada de *embedding*  utilizará todas as palavras no seu vocabulário associadas a 'gato' para construir o vetor de *embeddings*. Ela encontra "dimensões" ou características, como "ser vivo", "felino", "humano", "gênero", etc.


- Assim, a palavra 'gato' terá valores diferentes para cada dimensão/característica.



</div>

:::

::: {.column width="3%"}
:::

::: {.column width="47%"}

![](images/word-embeddings.png){style="margin: 0 0 0 0; width:1550px;"}

:::

::::

 -->
</div>
</section>
<section id="section-33" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Referências
</h2>
<hr>
<p><br></p>
<ul>
<li><p>Alexey Dosovitskiy et al., “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale” (2020). <a href="https://arxiv.org/abs/2010.11929" class="uri">https://arxiv.org/abs/2010.11929</a></p></li>
<li><p>Raschka, Sebastian. “Machine learning Q and AI: 30 essential questions and answers on machine learning and AI”. No Starch Press, 2024. <a href="https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch01/#table-of-contents" class="uri">https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch01/#table-of-contents</a></p></li>
<li><p>Johnson, Jeff, Matthijs Douze, and Hervé Jégou. “Billion-scale similarity search with GPUs.” IEEE Transactions on Big Data 7.3 (2019): 535-547. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8733051" class="uri">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8733051</a></p></li>
<li><p>Deng, Jia, et al.&nbsp;“Imagenet: A large-scale hierarchical image database.” 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5206848" class="uri">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5206848</a></p></li>
</ul>
</section>
<section class="slide level2">

<p><br> <br></p>
<hr>
<h1 style="text-align: center;">
<p>OBRIGADO!</p>
</h1>
<hr>
<div style="text-align: center">
<p>Slide produzido com <a href="https://quarto.org/">quarto</a></p>
</div>
<p><br> <br></p>
<div style="text-align:center;">
<p><img data-src="images/logo_ufpe_facepe.png" class="fixed" style="width:60.0%"></p>
</div>
<p><br> <br> <br> <br> <br></p>
</section>
<section class="slide level2">

<br> <br> <br> <br> <br> <br>
<hr>
<h2 style="color:#0000fd;text-align:center;">
Código python - Pré-Processamento
</h2>
<hr>
</section>
<section id="section-34" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Pré-processamento
</h2>
<hr>
<p>Aqui estou utilizando um Ambiente Conda para instalar as dependências necessárias, com <code>python 3.10</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href=""></a><span class="co"># Ambiente vit conda</span></span>
<span id="cb1-2"><a href=""></a>conda activate vit</span>
<span id="cb1-3"><a href=""></a></span>
<span id="cb1-4"><a href=""></a><span class="sc">!</span>pip install faiss<span class="sc">-</span>gpu</span>
<span id="cb1-5"><a href=""></a><span class="sc">!</span>pip install faiss<span class="sc">-</span>cpu</span>
<span id="cb1-6"><a href=""></a><span class="sc">!</span>pip install <span class="sc">-</span>r requirements.txt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lendo as bibliotecas necessárias:</p>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href=""></a><span class="co"># Lendo as bibliotecas</span></span>
<span id="cb2-2"><a href=""></a>from transformers import ViTImageProcessor, ViTModel</span>
<span id="cb2-3"><a href=""></a>import torch</span>
<span id="cb2-4"><a href=""></a>from PIL import Image</span>
<span id="cb2-5"><a href=""></a>import os</span>
<span id="cb2-6"><a href=""></a></span>
<span id="cb2-7"><a href=""></a>from rembg import remove</span>
<span id="cb2-8"><a href=""></a>import io</span>
<span id="cb2-9"><a href=""></a></span>
<span id="cb2-10"><a href=""></a>import pandas as pd</span>
<span id="cb2-11"><a href=""></a>import numpy as np</span>
<span id="cb2-12"><a href=""></a></span>
<span id="cb2-13"><a href=""></a>import faiss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="section-35" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Pré-processamento
</h2>
<hr>
<p>Função para ler imagens de um diretório, remover o background e salvar em outro repositório. Esta função também retorna uma lista com um ‘id’ para as imagens e o caminho da imagem original.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href=""></a><span class="co"># Pre-processar as imagens -</span></span>
<span id="cb3-2"><a href=""></a><span class="co"># Padronizar as imagens e remover background</span></span>
<span id="cb3-3"><a href=""></a></span>
<span id="cb3-4"><a href=""></a>def <span class="fu">processar_imagens_otimizado</span>(caminho_pasta_entrada, caminho_pasta_saida, <span class="at">largura=</span><span class="dv">640</span>, <span class="at">altura=</span><span class="dv">480</span>)<span class="sc">:</span></span>
<span id="cb3-5"><a href=""></a>    <span class="co"># (Mesma função processar_imagens_otimizado que você já tem)</span></span>
<span id="cb3-6"><a href=""></a>    <span class="cf">if</span> not <span class="fu">os.path.exists</span>(caminho_pasta_saida)<span class="sc">:</span></span>
<span id="cb3-7"><a href=""></a>        <span class="fu">os.makedirs</span>(caminho_pasta_saida)</span>
<span id="cb3-8"><a href=""></a>        <span class="fu">print</span>(f<span class="st">"Pasta de saída criada: {caminho_pasta_saida}"</span>)</span>
<span id="cb3-9"><a href=""></a></span>
<span id="cb3-10"><a href=""></a>    lista_indices_caminhos <span class="ot">=</span> []</span>
<span id="cb3-11"><a href=""></a>    indice_atual <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb3-12"><a href=""></a></span>
<span id="cb3-13"><a href=""></a>    <span class="cf">for</span> raiz, _, arquivos <span class="cf">in</span> <span class="fu">os.walk</span>(caminho_pasta_entrada)<span class="sc">:</span></span>
<span id="cb3-14"><a href=""></a>        <span class="fu">arquivos.sort</span>() <span class="co"># Garante uma ordem consistente</span></span>
<span id="cb3-15"><a href=""></a>        </span>
<span id="cb3-16"><a href=""></a>        <span class="cf">for</span> arquivo <span class="cf">in</span> arquivos<span class="sc">:</span></span>
<span id="cb3-17"><a href=""></a>            <span class="cf">if</span> <span class="fu">arquivo.lower</span>()<span class="fu">.endswith</span>((<span class="st">'.png'</span>, <span class="st">'.jpg'</span>, <span class="st">'.jpeg'</span>, <span class="st">'.webp'</span>, <span class="st">'.bmp'</span>, <span class="st">'.tiff'</span>))<span class="sc">:</span></span>
<span id="cb3-18"><a href=""></a>                caminho_completo_entrada <span class="ot">=</span> <span class="fu">os.path.join</span>(raiz, arquivo)</span>
<span id="cb3-19"><a href=""></a>                </span>
<span id="cb3-20"><a href=""></a>                <span class="fu">lista_indices_caminhos.append</span>({</span>
<span id="cb3-21"><a href=""></a>                    <span class="st">'indice'</span><span class="sc">:</span> indice_atual,</span>
<span id="cb3-22"><a href=""></a>                    <span class="st">'caminho_original'</span><span class="sc">:</span> caminho_completo_entrada</span>
<span id="cb3-23"><a href=""></a>                })</span>
<span id="cb3-24"><a href=""></a>                indice_atual <span class="sc">+</span><span class="er">=</span> <span class="dv">1</span></span>
<span id="cb3-25"><a href=""></a></span>
<span id="cb3-26"><a href=""></a>                nome_arquivo_sem_ext, extensao <span class="ot">=</span> <span class="fu">os.path.splitext</span>(arquivo)</span>
<span id="cb3-27"><a href=""></a>                extensao_saida <span class="ot">=</span> <span class="st">'.png'</span> <span class="cf">if</span> <span class="fu">extensao.lower</span>() <span class="sc">!=</span> <span class="st">'.png'</span> <span class="cf">else</span> extensao</span>
<span id="cb3-28"><a href=""></a>                nome_arquivo_saida <span class="ot">=</span> f<span class="st">"{nome_arquivo_sem_ext}_processado{extensao_saida}"</span></span>
<span id="cb3-29"><a href=""></a>                caminho_completo_saida <span class="ot">=</span> <span class="fu">os.path.join</span>(caminho_pasta_saida, nome_arquivo_saida)</span>
<span id="cb3-30"><a href=""></a></span>
<span id="cb3-31"><a href=""></a>                try<span class="sc">:</span></span>
<span id="cb3-32"><a href=""></a>                    img_original <span class="ot">=</span> <span class="fu">Image.open</span>(caminho_completo_entrada)</span>
<span id="cb3-33"><a href=""></a>                    img_redimensionada <span class="ot">=</span> <span class="fu">img_original.resize</span>((largura, altura), Image.LANCZOS)</span>
<span id="cb3-34"><a href=""></a></span>
<span id="cb3-35"><a href=""></a>                    buffer_img <span class="ot">=</span> <span class="fu">io.BytesIO</span>()</span>
<span id="cb3-36"><a href=""></a>                    <span class="fu">img_redimensionada.save</span>(buffer_img, <span class="at">format=</span><span class="st">'PNG'</span> <span class="cf">if</span> extensao_saida <span class="sc">==</span> <span class="st">'.png'</span> <span class="cf">else</span> img_original.format)</span>
<span id="cb3-37"><a href=""></a>                    <span class="fu">buffer_img.seek</span>(<span class="dv">0</span>)</span>
<span id="cb3-38"><a href=""></a></span>
<span id="cb3-39"><a href=""></a>                    bytes_entrada <span class="ot">=</span> <span class="fu">buffer_img.read</span>()</span>
<span id="cb3-40"><a href=""></a>                    bytes_saida <span class="ot">=</span> <span class="fu">remove</span>(bytes_entrada)</span>
<span id="cb3-41"><a href=""></a></span>
<span id="cb3-42"><a href=""></a>                    with <span class="fu">open</span>(caminho_completo_saida, <span class="st">'wb'</span>) as o<span class="sc">:</span></span>
<span id="cb3-43"><a href=""></a>                        <span class="fu">o.write</span>(bytes_saida)</span>
<span id="cb3-44"><a href=""></a>                    <span class="fu">print</span>(f<span class="st">"Processado e salvo: {caminho_completo_saida} (Original: {caminho_completo_entrada})"</span>)</span>
<span id="cb3-45"><a href=""></a></span>
<span id="cb3-46"><a href=""></a>                except FileNotFoundError<span class="sc">:</span></span>
<span id="cb3-47"><a href=""></a>                    <span class="fu">print</span>(f<span class="st">"Erro: Arquivo não encontrado: {caminho_completo_entrada}"</span>)</span>
<span id="cb3-48"><a href=""></a>                except Exception as e<span class="sc">:</span></span>
<span id="cb3-49"><a href=""></a>                    <span class="fu">print</span>(f<span class="st">"Erro ao processar {caminho_completo_entrada}: {e}"</span>)</span>
<span id="cb3-50"><a href=""></a></span>
<span id="cb3-51"><a href=""></a>    return lista_indices_caminhos</span>
<span id="cb3-52"><a href=""></a></span>
<span id="cb3-53"><a href=""></a></span>
<span id="cb3-54"><a href=""></a></span>
<span id="cb3-55"><a href=""></a></span>
<span id="cb3-56"><a href=""></a></span>
<span id="cb3-57"><a href=""></a><span class="co">#    </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="section-36" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Pré-processamento
</h2>
<hr>
<p><br></p>
<p>Processando as imagens e salvando os índices em um arquivo CSV:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href=""></a><span class="co"># Padronizar as imagens e remover background</span></span>
<span id="cb4-2"><a href=""></a>pasta_entrada <span class="ot">=</span> <span class="st">'database/natural_images'</span></span>
<span id="cb4-3"><a href=""></a>pasta_saida <span class="ot">=</span> <span class="st">'database/natural_images_without_bg'</span></span>
<span id="cb4-4"><a href=""></a>arquivo_csv_saida <span class="ot">=</span> <span class="st">'database/indices_imagens_preprocessadas.csv'</span></span>
<span id="cb4-5"><a href=""></a></span>
<span id="cb4-6"><a href=""></a><span class="co"># Processa as imagens e obtém a lista de índices/caminhos</span></span>
<span id="cb4-7"><a href=""></a>info_imagens <span class="ot">=</span> <span class="fu">processar_imagens_otimizado</span>(pasta_entrada, pasta_saida,</span>
<span id="cb4-8"><a href=""></a>  <span class="at">largura=</span><span class="dv">224</span>, <span class="at">altura=</span><span class="dv">224</span>)</span>
<span id="cb4-9"><a href=""></a><span class="co"># Salvando em CSV a lista de arquivos originais</span></span>
<span id="cb4-10"><a href=""></a>df <span class="ot">=</span> <span class="fu">pd.DataFrame</span>(info_imagens)</span>
<span id="cb4-11"><a href=""></a><span class="co"># index=False para não salvar o índice do DataFrame</span></span>
<span id="cb4-12"><a href=""></a><span class="fu">df.to_csv</span>(arquivo_csv_saida, <span class="at">index=</span>False, <span class="at">encoding=</span><span class="st">'utf-8'</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="section-37" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Pré-processamento
</h2>
<hr>
<p><br></p>
<p>Função para ler as imagens em uma lista:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href=""></a><span class="co"># Ler as imagens da pasta database/natural_images_without_bg</span></span>
<span id="cb5-2"><a href=""></a>def <span class="fu">ler_imagens</span>(lista_imagens, raiz)<span class="sc">:</span></span>
<span id="cb5-3"><a href=""></a>    imagens <span class="ot">=</span> []</span>
<span id="cb5-4"><a href=""></a></span>
<span id="cb5-5"><a href=""></a>    <span class="cf">for</span> arquivo <span class="cf">in</span> lista_imagens<span class="sc">:</span></span>
<span id="cb5-6"><a href=""></a>        nome_arquivo <span class="ot">=</span> <span class="fu">os.path.basename</span>(arquivo)</span>
<span id="cb5-7"><a href=""></a>        nome_arquivo_sem_ext, extensao <span class="ot">=</span> <span class="fu">os.path.splitext</span>(nome_arquivo)</span>
<span id="cb5-8"><a href=""></a>        extensao_saida <span class="ot">=</span> <span class="st">'.png'</span> <span class="cf">if</span> <span class="fu">extensao.lower</span>() <span class="sc">!=</span> <span class="st">'.png'</span> <span class="cf">else</span> extensao</span>
<span id="cb5-9"><a href=""></a>        nome_arquivo_saida <span class="ot">=</span> f<span class="st">"{nome_arquivo_sem_ext}_processado{extensao_saida}"</span></span>
<span id="cb5-10"><a href=""></a>        caminho_completo_saida <span class="ot">=</span> <span class="fu">os.path.join</span>(raiz, nome_arquivo_saida)</span>
<span id="cb5-11"><a href=""></a></span>
<span id="cb5-12"><a href=""></a>        try<span class="sc">:</span></span>
<span id="cb5-13"><a href=""></a>            img <span class="ot">=</span> <span class="fu">Image.open</span>(caminho_completo_saida)</span>
<span id="cb5-14"><a href=""></a>            img <span class="ot">=</span> <span class="fu">img.convert</span>(<span class="st">'RGB'</span>)</span>
<span id="cb5-15"><a href=""></a>            <span class="fu">imagens.append</span>(img)</span>
<span id="cb5-16"><a href=""></a>            <span class="fu">print</span>(f<span class="st">"Imagem lida: {caminho_completo_saida}"</span>)</span>
<span id="cb5-17"><a href=""></a>        except Exception as e<span class="sc">:</span></span>
<span id="cb5-18"><a href=""></a>            <span class="fu">print</span>(f<span class="st">"Não foi possível ler a imagem {caminho_completo_saida}: {e}"</span>)</span>
<span id="cb5-19"><a href=""></a>    return imagens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="section-38" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Pré-processamento
</h2>
<hr>
<p><br></p>
<p>Agora vamos carregar as imagens e criar os embeddings usando o ViT:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href=""></a><span class="co"># Lendo as Imagens para criação dos Embeddings:</span></span>
<span id="cb6-2"><a href=""></a>lista <span class="ot">=</span> <span class="fu">pd.read_csv</span>(<span class="st">"database/indices_imagens_preprocessadas.csv"</span>)</span>
<span id="cb6-3"><a href=""></a>lista_imagens <span class="ot">=</span> lista[<span class="st">"caminho_original"</span>]</span>
<span id="cb6-4"><a href=""></a>raiz <span class="ot">=</span> <span class="st">"database/natural_images_without_bg/"</span></span>
<span id="cb6-5"><a href=""></a>todas_as_imagens <span class="ot">=</span> <span class="fu">ler_imagens</span>(lista_imagens, raiz)</span>
<span id="cb6-6"><a href=""></a><span class="fu">print</span>(f<span class="st">"</span><span class="sc">\n</span><span class="st">Total de imagens encontradas: {len(todas_as_imagens)}"</span>)</span>
<span id="cb6-7"><a href=""></a></span>
<span id="cb6-8"><a href=""></a><span class="co"># Você pode acessar as imagens:</span></span>
<span id="cb6-9"><a href=""></a><span class="co"># todas_as_imagens[0] # Mostra a primeira imagem</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
<p>Lendo o ViT</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href=""></a><span class="co"># Lendo algoritmo ViT - PreTrained</span></span>
<span id="cb7-2"><a href=""></a>processor <span class="ot">=</span> <span class="fu">ViTImageProcessor.from_pretrained</span>(<span class="st">'google/vit-base-patch16-224'</span>)</span>
<span id="cb7-3"><a href=""></a>model <span class="ot">=</span> <span class="fu">ViTModel.from_pretrained</span>(<span class="st">'google/vit-base-patch16-224'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="section-39" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Pré-processamento
</h2>
<hr>
<p><br></p>
<p>Fazendo processo de normalização dos embeddings e criando o índice FAISS:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href=""></a><span class="co"># O processor transforma as imagens para o formato que o modelo espera (pixels, normalização, etc.)</span></span>
<span id="cb8-2"><a href=""></a><span class="co"># return_tensors="pt" garante que a saída seja um tensor PyTorch.</span></span>
<span id="cb8-3"><a href=""></a>inputs <span class="ot">=</span> <span class="fu">processor</span>(<span class="at">images=</span>todas_as_imagens[<span class="dv">0</span>], <span class="at">return_tensors=</span><span class="st">"pt"</span>)</span>
<span id="cb8-4"><a href=""></a><span class="co"># 2. Passar as imagens pré-processadas pelo modelo ViT</span></span>
<span id="cb8-5"><a href=""></a>with <span class="fu">torch.no_grad</span>()<span class="sc">:</span></span>
<span id="cb8-6"><a href=""></a>    outputs <span class="ot">=</span> <span class="fu">model</span>(<span class="sc">**</span>inputs)</span>
<span id="cb8-7"><a href=""></a><span class="co"># Pega o embedding</span></span>
<span id="cb8-8"><a href=""></a>cls_embedding <span class="ot">=</span> outputs.last_hidden_state[<span class="sc">:</span>, <span class="dv">0</span>, <span class="sc">:</span>]  <span class="co"># shape: [1, 768]</span></span>
<span id="cb8-9"><a href=""></a><span class="co"># Convertendo os logits para numpy (seu vetor de embeddings)</span></span>
<span id="cb8-10"><a href=""></a>img_embds <span class="ot">=</span> <span class="fu">cls_embedding.numpy</span>()</span>
<span id="cb8-11"><a href=""></a><span class="co"># Normalize os embeddings (para usar distância do cosseno)</span></span>
<span id="cb8-12"><a href=""></a>img_embds <span class="ot">=</span> img_embds <span class="sc">/</span> <span class="fu">np.linalg.norm</span>(img_embds, <span class="at">axis=</span><span class="dv">1</span>, <span class="at">keepdims=</span>True)</span>
<span id="cb8-13"><a href=""></a><span class="co"># Definindo o índice FAISS para busca vetorial</span></span>
<span id="cb8-14"><a href=""></a>d <span class="ot">=</span> img_embds.shape[<span class="dv">1</span>]  <span class="co"># dimensão dos embeddings</span></span>
<span id="cb8-15"><a href=""></a><span class="co"># Crie um índice  para o vetor de embeddings</span></span>
<span id="cb8-16"><a href=""></a>index <span class="ot">=</span> <span class="fu">faiss.IndexFlatIP</span>(d)</span>
<span id="cb8-17"><a href=""></a><span class="co"># Adicione os embeddings normalizados</span></span>
<span id="cb8-18"><a href=""></a><span class="fu">index.add</span>(img_embds)</span>
<span id="cb8-19"><a href=""></a><span class="fu">print</span>(f<span class="st">"Número de itens no índice: {index.ntotal}"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="section-40" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Pré-processamento
</h2>
<hr>
<p><br></p>
<p>Fazendo processo de normalização dos embeddings e criando o índice FAISS:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href=""></a><span class="cf">for</span> i <span class="cf">in</span> <span class="fu">range</span>(<span class="dv">1</span>,<span class="fu">len</span>(todas_as_imagens))<span class="sc">:</span></span>
<span id="cb9-2"><a href=""></a>    <span class="co"># O processor transforma as imagens para o formato que o modelo espera (pixels, normalização, etc.)</span></span>
<span id="cb9-3"><a href=""></a>    <span class="co"># return_tensors="pt" garante que a saída seja um tensor PyTorch.</span></span>
<span id="cb9-4"><a href=""></a>    inputs <span class="ot">=</span> <span class="fu">processor</span>(<span class="at">images=</span>todas_as_imagens[i], <span class="at">return_tensors=</span><span class="st">"pt"</span>)</span>
<span id="cb9-5"><a href=""></a>    <span class="co"># 2. Passar as imagens pré-processadas pelo modelo ViT</span></span>
<span id="cb9-6"><a href=""></a>    with <span class="fu">torch.no_grad</span>()<span class="sc">:</span></span>
<span id="cb9-7"><a href=""></a>        outputs <span class="ot">=</span> <span class="fu">model</span>(<span class="sc">**</span>inputs)</span>
<span id="cb9-8"><a href=""></a>    <span class="co"># Pega o embedding</span></span>
<span id="cb9-9"><a href=""></a>    cls_embedding <span class="ot">=</span> outputs.last_hidden_state[<span class="sc">:</span>, <span class="dv">0</span>, <span class="sc">:</span>]  <span class="co"># shape: [1, 768]</span></span>
<span id="cb9-10"><a href=""></a>    <span class="co"># Convertendo os logits para numpy (seu vetor de embeddings)</span></span>
<span id="cb9-11"><a href=""></a>    img_embds <span class="ot">=</span> <span class="fu">cls_embedding.numpy</span>()</span>
<span id="cb9-12"><a href=""></a></span>
<span id="cb9-13"><a href=""></a>    <span class="co"># Normalize os embeddings (para usar distância do cosseno)</span></span>
<span id="cb9-14"><a href=""></a>    img_embds <span class="ot">=</span> img_embds <span class="sc">/</span> <span class="fu">np.linalg.norm</span>(img_embds, <span class="at">axis=</span><span class="dv">1</span>, <span class="at">keepdims=</span>True)</span>
<span id="cb9-15"><a href=""></a></span>
<span id="cb9-16"><a href=""></a>    <span class="co"># Adicione os embeddings normalizados</span></span>
<span id="cb9-17"><a href=""></a>    <span class="fu">index.add</span>(img_embds)</span>
<span id="cb9-18"><a href=""></a>    <span class="fu">print</span>(f<span class="st">"Número de itens no índice: {index.ntotal}"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="section-41" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Pré-processamento
</h2>
<hr>
<p><br></p>
<p>Salvando as imagens no FAISS:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href=""></a><span class="co"># Salvando os indices de embeddings</span></span>
<span id="cb10-2"><a href=""></a><span class="fu">faiss.write_index</span>(index, <span class="st">'faiss_index_embeddings/vector_databases_faiss_index.index'</span>)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Para carregar o índice posteriormente:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href=""></a><span class="co"># Lendo os índices</span></span>
<span id="cb11-2"><a href=""></a>index <span class="ot">=</span> <span class="fu">faiss.read_index</span>(<span class="st">'faiss_index_embeddings/vector_databases_faiss_index.index'</span>)</span>
<span id="cb11-3"><a href=""></a></span>
<span id="cb11-4"><a href=""></a><span class="fu">print</span>(f<span class="st">"Número de itens no índice: {index.ntotal}"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="section-42" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Pré-processamento
</h2>
<hr>
<div class="columns">
<div class="column" style="width:20%;">
<div style="text-align:right;">
<p><img data-src="images/remocao_background/dog_0530_224.jpg" class="fixed" style="width:100.0%"></p>
<p style="margin-top:-20pt; font-size:12pt">
Dimensões: 224px vs 224px
</p>
</div>
<div style="text-align:right;">
<div style="text-align:right;margin-right:50pt;">
<p><span class="math inline">\(\Uparrow\)</span></p>
</div>
<p><img data-src="images/remocao_background/dog_0530.jpg" class="fixed" style="width:100.0%"></p>
<p style="margin-top:-20pt; font-size:12pt">
Dimensões: 238px vs 280px
</p>
</div>
</div><div class="column" style="width:10%;">
<div style="margin-top:50pt; text-align:center;">
<p><span class="math inline">\(\Rightarrow\)</span></p>
</div>
</div><div class="column" style="width:20%;">
<div style="text-align:left;">
<p><img data-src="images/remocao_background/dog_0530_processado.png" class="fixed" style="width:100.0%"></p>
<p style="margin-top:-20pt; font-size:12pt">
Dimensões: 224px vs 224px
</p>
</div>
<div style="text-align:left;">
<div style="text-align:left;margin-left:50pt;">
<p><span class="math inline">\(\Downarrow\)</span></p>
</div>
<p><img data-src="images/remocao_background/dog_0530_processado_black.jpg" class="fixed" style="width:100.0%"></p>
<p style="margin-top:-20pt; font-size:12pt">
Dimensões: 238px vs 280px
</p>
</div>
</div><div class="column" style="width:5%;">
<div style="bottom: 120px; position: fixed; text-align:center;">
<p><span class="math inline">\(\Rightarrow\)</span></p>
</div>
</div><div class="column" style="width:40%;">
<div style="bottom: 50px; position: fixed; text-align:left;">
<p><img data-src="images/remocao_background/array_embeddings.png" class="fixed" style="width:100.0%"></p>
<p style="margin-top:-20pt; font-size:12pt; text-align:center;">
Dimensões: 1 vs 768
</p>
</div>
</div></div>
</section>
<section class="slide level2">

<br> <br> <br> <br> <br> <br>
<hr>
<h2 style="color:#0000fd;text-align:center;">
Código python - Processamento
</h2>
<hr>
</section>
<section id="section-43" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Processamento
</h2>
<hr>
<p><br></p>
<p>Lendo as bibliotecas necessárias:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href=""></a><span class="co"># Lendo as bibliotecas</span></span>
<span id="cb12-2"><a href=""></a>from transformers import ViTImageProcessor, ViTModel</span>
<span id="cb12-3"><a href=""></a>from PIL import Image</span>
<span id="cb12-4"><a href=""></a>import torch</span>
<span id="cb12-5"><a href=""></a></span>
<span id="cb12-6"><a href=""></a>from rembg import remove</span>
<span id="cb12-7"><a href=""></a></span>
<span id="cb12-8"><a href=""></a>import pandas as pd</span>
<span id="cb12-9"><a href=""></a>import numpy as np</span>
<span id="cb12-10"><a href=""></a></span>
<span id="cb12-11"><a href=""></a>import faiss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="section-44" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Processamento
</h2>
<hr>
<p><br></p>
<p>Lendo a lista de Índices e Caminhos das imagens pré-processadas:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href=""></a>lista <span class="ot">=</span> <span class="fu">pd.read_csv</span>(<span class="st">"database/indices_imagens_preprocessadas.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Importando o FAISS e carregando o índice de embeddings:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href=""></a><span class="co"># Carrega o índice de um arquivo</span></span>
<span id="cb14-2"><a href=""></a><span class="co"># Ler o índice do arquivo</span></span>
<span id="cb14-3"><a href=""></a>index <span class="ot">=</span> <span class="fu">faiss.read_index</span>(<span class="st">'faiss_index_embeddings/vector_databases_faiss_index.index'</span>)  </span>
<span id="cb14-4"><a href=""></a></span>
<span id="cb14-5"><a href=""></a><span class="fu">print</span>(f<span class="st">"Número de itens no índice: {index.ntotal}"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lendo o ViT:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href=""></a><span class="co"># Lendo algoritmo ViT - PreTrained</span></span>
<span id="cb15-2"><a href=""></a>processor <span class="ot">=</span> <span class="fu">ViTImageProcessor.from_pretrained</span>(<span class="st">'google/vit-base-patch16-224'</span>)</span>
<span id="cb15-3"><a href=""></a>model <span class="ot">=</span> <span class="fu">ViTModel.from_pretrained</span>(<span class="st">'google/vit-base-patch16-224'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="section-45" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Processamento
</h2>
<hr>
<p>Lendo uma nova imagem para buscar similaridades:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href=""></a><span class="co"># Processar a nova imagem</span></span>
<span id="cb16-2"><a href=""></a>imagem <span class="ot">=</span> <span class="fu">Image.open</span>(<span class="st">"database/natural_images2/airplane/airplane_0004.jpg"</span>)</span>
<span id="cb16-3"><a href=""></a>img_redimensionada <span class="ot">=</span> <span class="fu">imagem.resize</span>((<span class="dv">224</span>, <span class="dv">224</span>), Image.LANCZOS) <span class="co"># Usando LANCZOS para melhor qualidade</span></span>
<span id="cb16-4"><a href=""></a>bytes_saida <span class="ot">=</span> <span class="fu">remove</span>(img_redimensionada)</span>
<span id="cb16-5"><a href=""></a>novaimagem <span class="ot">=</span> <span class="fu">bytes_saida.convert</span>(<span class="st">'RGB'</span>)</span>
<span id="cb16-6"><a href=""></a></span>
<span id="cb16-7"><a href=""></a>novaimagem</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div style="text-align:center;">
<p><img data-src="images/aviao1.png" class="fixed" style="width:35.0%"></p>
</div>
</section>
<section id="section-46" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Processamento
</h2>
<hr>
<p>Criando os embeddings da nova imagem:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href=""></a>inputs <span class="ot">=</span> <span class="fu">processor</span>(<span class="at">images=</span>novaimagem, <span class="at">return_tensors=</span><span class="st">"pt"</span>)</span>
<span id="cb17-2"><a href=""></a>with <span class="fu">torch.no_grad</span>()<span class="sc">:</span></span>
<span id="cb17-3"><a href=""></a>    outputs <span class="ot">=</span> <span class="fu">model</span>(<span class="sc">**</span>inputs)</span>
<span id="cb17-4"><a href=""></a>cls_embedding <span class="ot">=</span> outputs.last_hidden_state[<span class="sc">:</span>, <span class="dv">0</span>, <span class="sc">:</span>]  <span class="co"># shape: [1, 768]</span></span>
<span id="cb17-5"><a href=""></a>nova_embedding <span class="ot">=</span> <span class="fu">cls_embedding.numpy</span>()</span>
<span id="cb17-6"><a href=""></a><span class="co"># Normalize os embeddings (para usar distância do cosseno)</span></span>
<span id="cb17-7"><a href=""></a>nova_embedding <span class="ot">=</span> nova_embedding <span class="sc">/</span> <span class="fu">np.linalg.norm</span>(nova_embedding, <span class="at">axis=</span><span class="dv">1</span>, <span class="at">keepdims=</span>True)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Buscando as 4 imagens mais similares:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href=""></a><span class="co"># Buscando as k imagens mais próximas no índice</span></span>
<span id="cb18-2"><a href=""></a>k <span class="ot">=</span> <span class="dv">4</span>  <span class="co"># Número de imagens semelhantes a buscar</span></span>
<span id="cb18-3"><a href=""></a>distances, indices <span class="ot">=</span> <span class="fu">index.search</span>(nova_embedding, k)</span>
<span id="cb18-4"><a href=""></a><span class="co"># Imagens Retornadas</span></span>
<span id="cb18-5"><a href=""></a>imagens_retornadas <span class="ot">=</span> [i <span class="cf">for</span> i <span class="cf">in</span> indices[<span class="dv">0</span>]]</span>
<span id="cb18-6"><a href=""></a><span class="fu">print</span>(imagens_retornadas)</span>
<span id="cb18-7"><a href=""></a></span>
<span id="cb18-8"><a href=""></a><span class="co"># Valores da distância do Cosseno</span></span>
<span id="cb18-9"><a href=""></a>distances</span>
<span id="cb18-10"><a href=""></a></span>
<span id="cb18-11"><a href=""></a><span class="co"># Criando uma sublista apenas com as imagens retornadas</span></span>
<span id="cb18-12"><a href=""></a>sublista <span class="ot">=</span> lista[lista[<span class="st">'indice'</span>]<span class="fu">.isin</span>(imagens_retornadas)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="section-47" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Processamento
</h2>
<hr>
<p><br></p>
<p>Criando uma array de imagens retornadas para plotar como uma grade:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href=""></a><span class="co"># Criando uma array com das imagens</span></span>
<span id="cb19-2"><a href=""></a>novasimagem <span class="ot">=</span> []</span>
<span id="cb19-3"><a href=""></a></span>
<span id="cb19-4"><a href=""></a><span class="cf">for</span> i <span class="cf">in</span> <span class="fu">range</span>(<span class="fu">len</span>(sublista))<span class="sc">:</span></span>
<span id="cb19-5"><a href=""></a>    imgpath <span class="ot">=</span> sublista.iloc[i][<span class="dv">1</span>]</span>
<span id="cb19-6"><a href=""></a>    <span class="co"># Processar a nova imagem</span></span>
<span id="cb19-7"><a href=""></a>    imagem <span class="ot">=</span> <span class="fu">Image.open</span>(imgpath)</span>
<span id="cb19-8"><a href=""></a>    <span class="co"># Usando LANCZOS para melhor qualidade</span></span>
<span id="cb19-9"><a href=""></a>    img_redimensionada <span class="ot">=</span> <span class="fu">imagem.resize</span>((<span class="dv">640</span>, <span class="dv">480</span>), Image.LANCZOS) </span>
<span id="cb19-10"><a href=""></a>    novaimagem <span class="ot">=</span> <span class="fu">img_redimensionada.convert</span>(<span class="st">'RGB'</span>)</span>
<span id="cb19-11"><a href=""></a>    <span class="fu">novasimagem.append</span>(novaimagem)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="section-48" class="slide level2">
<h2></h2>
<h2 style="color:#0000fd;">
Código python - Processamento
</h2>
<hr>
<p><br></p>
<div class="columns">
<div class="column" style="width:48%;">
<p>Plotando as imagens retornadas:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href=""></a>largura_celula <span class="ot">=</span> <span class="dv">640</span></span>
<span id="cb20-2"><a href=""></a>altura_celula <span class="ot">=</span> <span class="dv">480</span></span>
<span id="cb20-3"><a href=""></a></span>
<span id="cb20-4"><a href=""></a>largura_total <span class="ot">=</span> largura_celula <span class="sc">*</span> <span class="dv">2</span></span>
<span id="cb20-5"><a href=""></a>altura_total <span class="ot">=</span> altura_celula <span class="sc">*</span> <span class="dv">2</span></span>
<span id="cb20-6"><a href=""></a>grade_final <span class="ot">=</span> <span class="fu">Image.new</span>(<span class="st">'RGB'</span></span>
<span id="cb20-7"><a href=""></a>  (largura_total, altura_total))</span>
<span id="cb20-8"><a href=""></a></span>
<span id="cb20-9"><a href=""></a><span class="fu">grade_final.paste</span>(novasimagem[<span class="dv">0</span>],</span>
<span id="cb20-10"><a href=""></a>  (<span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb20-11"><a href=""></a><span class="fu">grade_final.paste</span>(novasimagem[<span class="dv">1</span>],</span>
<span id="cb20-12"><a href=""></a>  (largura_celula, <span class="dv">0</span>))</span>
<span id="cb20-13"><a href=""></a><span class="fu">grade_final.paste</span>(novasimagem[<span class="dv">2</span>],</span>
<span id="cb20-14"><a href=""></a>  (<span class="dv">0</span>, altura_celula))</span>
<span id="cb20-15"><a href=""></a><span class="fu">grade_final.paste</span>(novasimagem[<span class="dv">3</span>],</span>
<span id="cb20-16"><a href=""></a>  (largura_celula, altura_celula))</span>
<span id="cb20-17"><a href=""></a></span>
<span id="cb20-18"><a href=""></a><span class="co"># Mostra a imagem da grade na tela</span></span>
<span id="cb20-19"><a href=""></a><span class="fu">print</span>(<span class="st">"Exibindo a grade de imagens na tela..."</span>)</span>
<span id="cb20-20"><a href=""></a><span class="fu">grade_final.show</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div><div class="column" style="width:48%;">
<div style="text-align:center; margin-top:-50px;">
<p><img data-src="images/aviao1.png" class="fixed" style="width:50.0%"></p>
</div>
<div style="text-align:center;">
<p><img data-src="images/aviao2.png" class="fixed" style="width:80.0%"></p>
</div>
</div></div>

</section>

    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="index_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="index_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="index_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="index_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="index_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="index_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'concave',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'convex',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>